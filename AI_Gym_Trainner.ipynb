{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrawal-harsh2003/AI_Gym_Trainner/blob/HPE/AI_Gym_Trainner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVmaJR2vd9q8"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWJlauZBd69C"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision datasets transformers matplotlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import ResNetConfig, ResNetModel, ResNetForImageClassification, AutoImageProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwqDskxCB1nI"
      },
      "source": [
        "Import Coco-Wholebody dataset using Hugging Face API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1PC7ROoRAD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"harsh-7070/COCO-Wholebody-annotated\")\n",
        "print(ds['train'][0].keys())\n",
        "print(ds.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supporting code to find Num of Keypoints in the dataset."
      ],
      "metadata": {
        "id": "e43xFck-I-51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEHLKFQ9ZHw5"
      },
      "outputs": [],
      "source": [
        "# def find_max_keypoints(dataset):\n",
        "#     max_keypoints = 0\n",
        "#     count = 0\n",
        "#     for item in dataset:\n",
        "#         keypoints = item['objects']['keypoints']  # Assuming the keypoints are in this field\n",
        "#         num_keypoints = len(keypoints)\n",
        "#         max_keypoints = max(max_keypoints, num_keypoints)\n",
        "#         count += 1\n",
        "#         if count % 1000 == 0:\n",
        "#             print(count)\n",
        "\n",
        "#     print(max_keypoints)\n",
        "\n",
        "# find_max_keypoints(ds['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom functions to help normalize the Dataset."
      ],
      "metadata": {
        "id": "5d0bcAbyKXi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL8sOryKUooJ"
      },
      "outputs": [],
      "source": [
        "#data augmentation.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
        "])\n",
        "\n",
        "#Padding the keypoints to create equal length across the annotation.\n",
        "def pad_keypoints(keypoints_tensor, target_length, padding_value=-1):\n",
        "    current_length = keypoints_tensor.shape[0]\n",
        "\n",
        "    if current_length < target_length:\n",
        "        padding_size = target_length - current_length\n",
        "        padding = torch.full((padding_size, 3), padding_value)\n",
        "        padded_keypoints = torch.cat((keypoints_tensor, padding), dim=0)\n",
        "    else:\n",
        "        padded_keypoints = keypoints_tensor\n",
        "    return padded_keypoints\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    pixel_values_list = [item[0] for item in batch]\n",
        "    keypoints_list = [item[1] for item in batch]\n",
        "\n",
        "    pixel_values_padded = torch.stack(pixel_values_list, dim=0)\n",
        "    keypoints_padded = torch.nn.utils.rnn.pad_sequence(keypoints_list, batch_first=True, padding_value=-1)\n",
        "\n",
        "    return pixel_values_padded, keypoints_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Dataset Class."
      ],
      "metadata": {
        "id": "NoCHNS-KKNdC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edUXkvK0k_JC"
      },
      "outputs": [],
      "source": [
        "class COCOWholeBodyDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        image = item['image']\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        keypoints = item['objects']['keypoints']\n",
        "        keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32).view(-1, 3)\n",
        "\n",
        "        #Pad the keypoints tensor\n",
        "        # keypoints_tensor = pad_keypoints(keypoints_tensor, 20)\n",
        "        keypoints_tensor = pad_keypoints(keypoints_tensor, 2660)\n",
        "\n",
        "        # Process the image using the image processor\n",
        "        inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze()\n",
        "\n",
        "        if self.transform:\n",
        "            pixel_values = self.transform(pixel_values)\n",
        "\n",
        "        return pixel_values, keypoints_tensor\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 15\n",
        "batch_size = 512\n",
        "learning_rate = 0.001\n",
        "num_keypoints = 2660 #20*133 (20 = max no. of annotation merged as one after pre-processing the .jsonl before uploadin to COCO Hub)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset creation using pre-existing split and data loader."
      ],
      "metadata": {
        "id": "rU-87vCHKuIa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWl6vM_rjg6M"
      },
      "outputs": [],
      "source": [
        "train_dataset = COCOWholeBodyDataset(ds['train'], transform=data_transforms)\n",
        "val_dataset = COCOWholeBodyDataset(ds['validation'])\n",
        "\n",
        "#data loaders with collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaS7SjJ3pvJ5"
      },
      "source": [
        "Model Initialization and Configuration leveraging Pre-Trained model from Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv4diU6p3mXY"
      },
      "outputs": [],
      "source": [
        "class ResNetForImageClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(ResNetForImageClassification, self).__init__()\n",
        "        self.config = config\n",
        "        self.resnet_backbone = torchvision.models.resnet50(pretrained=True)\n",
        "        self.resnet_backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, num_keypoints * 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet_backbone(x)\n",
        "\n",
        "        if len(x.shape) == 4:\n",
        "            x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        elif len(x.shape) == 2:\n",
        "            x = x.flatten(1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected tensor shape: {x.shape}\")\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "config = ResNetConfig(\n",
        "    num_channels=3,\n",
        "    embedding_size=64,\n",
        "    hidden_sizes=[256, 512, 1024, 2048],\n",
        "    depths=[3, 4, 6, 3],\n",
        "    layer_type=\"bottleneck\",\n",
        "    hidden_act=\"relu\",\n",
        "    num_labels = num_keypoints * 2\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNetForImageClassification(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd3kAv3gfQ2y"
      },
      "outputs": [],
      "source": [
        "# for images, keypoints in train_loader:\n",
        "#     images = images.to(device)\n",
        "#     keypoints = keypoints.to(device)\n",
        "#     print(images.shape)\n",
        "#     print(keypoints.shape)\n",
        "#     output = model(images)\n",
        "#     print(output.shape)\n",
        "#     break\n",
        "# torch.Size([32, 3, 224, 224])\n",
        "# torch.Size([32, 238, 3])\n",
        "# torch.Size([32, 476])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLWQtdL5uGmU"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utjqy8lACxls"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model_path = \"Best_HPE.pth\"\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "    for batch_idx, (images, keypoints) in enumerate(tqdm(train_loader)):\n",
        "        images = images.to(device)\n",
        "        keypoints = keypoints.to(device)\n",
        "\n",
        "        keypoints = keypoints[:, :, :2]\n",
        "        # keypoints = keypoints.view(images.size(0), -1)  # Flatten to [batch_size, 238 * 2]\n",
        "        keypoints = keypoints.reshape(images.size(0), -1)\n",
        "\n",
        "        output = model(images)\n",
        "\n",
        "        mask = (keypoints != -1).float()\n",
        "        loss = criterion(output, keypoints)\n",
        "        masked_loss = loss * mask\n",
        "        loss = masked_loss.sum() / mask.sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "\n",
        "    avg_loss = running_loss / total_samples\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "\n",
        "val_loss = 0.0\n",
        "val_samples = 0\n",
        "\n",
        "with torch.no_grad(): for batch_idx, (images, keypoints) in enumerate(tqdm(val_loader)):\n",
        "  images = images.to(device)\n",
        "  keypoints = keypoints.to(device)[:, :, :2]\n",
        "  keypoints = keypoints.reshape(images.size(0), -1)\n",
        "\n",
        "  output = model(images)\n",
        "\n",
        "  mask = (keypoints != -1).float()\n",
        "  loss = criterion(output, keypoints)\n",
        "  masked_loss = loss * mask\n",
        "  loss = masked_loss.sum() / mask.sum()\n",
        "\n",
        "  val_loss += loss.item() * images.size(0)\n",
        "  val_samples += images.size(0)\n",
        "\n",
        "avg_val_loss = val_loss / val_samples\n",
        "print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Save the model if the validation loss is the best we've seen so far\n",
        "if avg_val_loss < best_val_loss:\n",
        "  best_val_loss = avg_val_loss torch.save(model.state_dict(), best_model_path)\n",
        "  print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOROjJPgH/5MayS+WB3YSLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}